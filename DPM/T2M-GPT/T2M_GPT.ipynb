{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrlqSWBT2A-n"
      },
      "source": [
        "# Initial Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8yYELhE0aoz",
        "outputId": "a1272b05-9f3b-4343-e4ce-9e7ef5b0dd40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-7cfkvl9n\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-7cfkvl9n\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=2cf88ec0884550b7682585303312fa392ffffe0caa688ae467d1d23401deabaa\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-y_i8ogdc/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.1\n",
            "Collecting git+https://github.com/nghorbani/human_body_prior\n",
            "  Cloning https://github.com/nghorbani/human_body_prior to /tmp/pip-req-build-5pmqkz25\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/nghorbani/human_body_prior /tmp/pip-req-build-5pmqkz25\n",
            "  Resolved https://github.com/nghorbani/human_body_prior to commit 4c246d8a83ce16d3cff9c79dcf04d81fa440a6bc\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: human-body-prior\n",
            "  Building wheel for human-body-prior (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for human-body-prior: filename=human_body_prior-2.2.2.0-py3-none-any.whl size=7610628 sha256=442d53cc16bff473312c1e90ccf8d74edf9044635bfd70ee95827484c1f9dd1e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lftdy5t8/wheels/5d/e6/4e/82c96e6eadbaa21f1c3099d21df9e16a56581c782aeb10e17a\n",
            "Successfully built human-body-prior\n",
            "Installing collected packages: human-body-prior\n",
            "Successfully installed human-body-prior-2.2.2.0\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (6.3.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=5.0.0->ipykernel)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel) (4.8.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel) (5.4.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel) (23.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel) (2.8.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel) (0.8.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.0->jupyter-client->ipykernel) (3.11.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.0.0->ipykernel) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel) (0.2.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client->ipykernel) (1.16.0)\n",
            "Installing collected packages: jedi\n",
            "Successfully installed jedi-0.19.1\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.23.5)\n",
            "Collecting smplx\n",
            "  Downloading smplx-0.1.28-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from smplx) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.0.1.post2 in /usr/local/lib/python3.10/dist-packages (from smplx) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1.post2->smplx) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1.post2->smplx) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1.post2->smplx) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1.post2->smplx) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1.post2->smplx) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1.post2->smplx) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1.post2->smplx) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.1.post2->smplx) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.1.post2->smplx) (1.3.0)\n",
            "Installing collected packages: smplx\n",
            "Successfully installed smplx-0.1.28\n",
            "Collecting chumpy\n",
            "  Downloading chumpy-0.70.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from chumpy) (1.11.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from chumpy) (1.16.0)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy>=0.13.0->chumpy) (1.23.5)\n",
            "Building wheels for collected packages: chumpy\n",
            "  Building wheel for chumpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chumpy: filename=chumpy-0.70-py3-none-any.whl size=58263 sha256=2441264c15affd76aefadf0cc1351eb9070fa978446fd2f2824b1c0e13a7e5cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/c1/ef/29ba7be03653a29ef6f2c3e1956d6c4d8877f2b243af411db1\n",
            "Successfully built chumpy\n",
            "Installing collected packages: chumpy\n",
            "Successfully installed chumpy-0.70\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install git+https://github.com/nghorbani/human_body_prior\n",
        "!pip install --user ipykernel\n",
        "!pip install h5py\n",
        "!pip install smplx\n",
        "!pip install chumpy\n",
        "# !pip install matplotlib==3.4.3\n",
        "# !pip install matplotlib-inline==0.1.2\n",
        "# !pip install -r t2m_gpt_requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKbzAxSx0lrj",
        "outputId": "ec11950b-42fb-4ea7-8417-7c123ec1c63c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'T2M-GPT'...\n",
            "remote: Enumerating objects: 153, done.\u001b[K\n",
            "remote: Counting objects: 100% (51/51), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 153 (delta 34), reused 25 (delta 25), pack-reused 102\u001b[K\n",
            "Receiving objects: 100% (153/153), 19.53 MiB | 22.76 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n",
            "/content/T2M-GPT\n",
            "/content/T2M-GPT\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Mael-zys/T2M-GPT.git\n",
        "%cd /content/T2M-GPT/\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShGis1sv0puC",
        "outputId": "07ced07c-400a-4812-c638-8e251ec5ea68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# @title Connect to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eWCJnz1n0wdj"
      },
      "outputs": [],
      "source": [
        "import sys, warnings, argparse, os, shutil, random\n",
        "import tqdm, chumpy, smplx, h5py\n",
        "import io, imageio\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import mpl_toolkits.mplot3d.axes3d as p3\n",
        "import visualization.plot_3d_global as plot_3d\n",
        "import numpy as np\n",
        "import torch\n",
        "import clip\n",
        "import models.vqvae as vqvae\n",
        "import models.t2m_trans as trans\n",
        "import utils.rotation_conversions as geometry\n",
        "import options.option_transformer as option_trans\n",
        "\n",
        "from PIL import Image\n",
        "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
        "from textwrap import wrap\n",
        "from tqdm import tqdm\n",
        "from utils.motion_process import recover_from_ric\n",
        "from visualize.joints2smpl.src import config\n",
        "from visualize.joints2smpl.src.smplify import SMPLify3D\n",
        "from models.rotation2xyz import Rotation2xyz\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ToSM86V2D0u",
        "outputId": "e76b2583-251f-4ea4-fb8a-2022dc6c5e0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed: 8\n"
          ]
        }
      ],
      "source": [
        "# @title Set argument\n",
        "parser = argparse.ArgumentParser(description='Optimal Transport AutoEncoder training for Amass',\n",
        "                                     add_help=True,\n",
        "                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "\n",
        "args = parser\n",
        "args.dataname = 't2m'\n",
        "args.resume_pth = 'pretrained/VQVAE/net_last.pth'\n",
        "args.resume_trans = 'pretrained/VQTransformer_corruption05/net_best_fid.pth'\n",
        "args.down_t = 2\n",
        "args.depth = 3\n",
        "args.block_size = 51\n",
        "args.batch_size = 128\n",
        "args.fps= [20]\n",
        "args.seq_len = 64\n",
        "args.total_iter = 100000\n",
        "args.warm_up_iter = 1000\n",
        "args.lr = 2e-4\n",
        "args.lr_scheduler = [60000]\n",
        "args.gamma = 0.05\n",
        "args.weight_decay = 1e-6\n",
        "args.decay_option = \"all\"\n",
        "args.optimizer = \"adamw\"\n",
        "args.code_dim = 512\n",
        "args.nb_code = 512\n",
        "args.mu = 0.99\n",
        "args.stride_t = 2\n",
        "args.width = 512\n",
        "args.dilation_growth_rate = 3\n",
        "args.output_emb_width = 512\n",
        "args.vq_act = \"relu\"\n",
        "args.embed_dim_gpt = 512\n",
        "args.clip_dim = 512\n",
        "args.num_layers = 2\n",
        "args.n_head_gpt = 8\n",
        "args.ff_rate = 4\n",
        "args.drop_out_rate = 0.1\n",
        "args.quantizer = \"ema_reset\"\n",
        "args.quantbeta = 1.0\n",
        "args.out_dir = 'output_GPT_Final/'\n",
        "args.exp_name = \"exp_debug\"\n",
        "args.vq_name = \"exp_debug\"\n",
        "args.print_iter = 200\n",
        "args.eval_iter = 5000\n",
        "args.seed = 123\n",
        "args.if_maxtest = \"store_true\"\n",
        "args.pkeep = 1.0\n",
        "args.seed = random.randint(1, 10)\n",
        "print(\"Seed:\", args.seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1spNaz5N2jnB",
        "outputId": "06d6604f-4958-4ddd-8331-67e4ea4834f5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 131MiB/s]\n"
          ]
        }
      ],
      "source": [
        "## load clip model and datasets\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=torch.device('cuda'), jit=False, download_root='./')  # Must set jit=False for training\n",
        "clip.model.convert_weights(clip_model)  # Actually this line is unnecessary since clip by default already on float16\n",
        "clip_model.eval()\n",
        "for p in clip_model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "net = vqvae.HumanVQVAE(args, ## use args to define different parameters in different quantizers\n",
        "                       args.nb_code,\n",
        "                       args.code_dim,\n",
        "                       args.output_emb_width,\n",
        "                       args.down_t,\n",
        "                       args.stride_t,\n",
        "                       args.width,\n",
        "                       args.depth,\n",
        "                       args.dilation_growth_rate)\n",
        "\n",
        "trans_encoder = trans.Text2Motion_Transformer(num_vq=args.nb_code,\n",
        "                                embed_dim=1024,\n",
        "                                clip_dim=args.clip_dim,\n",
        "                                block_size=args.block_size,\n",
        "                                num_layers=9,\n",
        "                                n_head=16,\n",
        "                                drop_out_rate=args.drop_out_rate,\n",
        "                                fc_rate=args.ff_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsLWBJlD2l6P",
        "outputId": "8fd4fb07-30a9-4ffc-c0d4-9f22f9c2f48e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The pretrained model files will be stored in the 'pretrained' folder\n",
            "\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1LaOvwypF-jM2Axnq5dc-Iuvv3w_G-WDE\n",
            "To: /content/T2M-GPT/pretrained/VQTrans_pretrained.zip\n",
            "100% 994M/994M [00:15<00:00, 64.6MB/s]\n",
            "Archive:  VQTrans_pretrained.zip\n",
            "  inflating: VQTransformer_corruption05/net_best_fid.pth  \n",
            "  inflating: VQTransformer_corruption05/run.log  \n",
            "  inflating: VQVAE/net_best_fid.pth  \n",
            "  inflating: VQVAE/net_last.pth      \n",
            "  inflating: VQVAE/run.log           \n",
            "Cleaning\n",
            "\n",
            "Downloading done!\n",
            "Downloading extractors\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1o7RTDQcToJjTm9_mNWTyzvZvjTWpZfug\n",
            "To: /content/T2M-GPT/checkpoints/t2m.zip\n",
            "100% 1.22G/1.22G [00:20<00:00, 58.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1KNU8CsMAnxFrwopKBBkC8jEULGLPBHQp\n",
            "To: /content/T2M-GPT/checkpoints/kit.zip\n",
            "100% 705M/705M [00:13<00:00, 52.9MB/s]\n",
            "Archive:  t2m.zip\n",
            "   creating: t2m/\n",
            "   creating: t2m/Comp_v6_KLD005/\n",
            "   creating: t2m/Comp_v6_KLD005/meta/\n",
            "  inflating: t2m/Comp_v6_KLD005/meta/mean.npy  \n",
            "  inflating: t2m/Comp_v6_KLD005/meta/std.npy  \n",
            "  inflating: t2m/Comp_v6_KLD005/opt.txt  \n",
            "   creating: t2m/M2T_EL4_DL4_NH8_PS/\n",
            "   creating: t2m/M2T_EL4_DL4_NH8_PS/meta/\n",
            "  inflating: t2m/M2T_EL4_DL4_NH8_PS/meta/mean.npy  \n",
            "  inflating: t2m/M2T_EL4_DL4_NH8_PS/meta/std.npy  \n",
            "   creating: t2m/M2T_EL4_DL4_NH8_PS/model/\n",
            "  inflating: t2m/M2T_EL4_DL4_NH8_PS/model/finest.tar  \n",
            "  inflating: t2m/M2T_EL4_DL4_NH8_PS/opt.txt  \n",
            "   creating: t2m/T2M_Seq2Seq_NML1_Ear_SME0_N/\n",
            "   creating: t2m/T2M_Seq2Seq_NML1_Ear_SME0_N/meta/\n",
            "   creating: t2m/T2M_Seq2Seq_NML1_Ear_SME0_N/model/\n",
            "  inflating: t2m/T2M_Seq2Seq_NML1_Ear_SME0_N/model/finest.tar  \n",
            "  inflating: t2m/T2M_Seq2Seq_NML1_Ear_SME0_N/opt.txt  \n",
            "   creating: t2m/text_mot_match/\n",
            "   creating: t2m/text_mot_match/eval/\n",
            "  inflating: t2m/text_mot_match/eval/E005.txt  \n",
            "  inflating: t2m/text_mot_match/eval/E010.txt  \n",
            "  inflating: t2m/text_mot_match/eval/E015.txt  \n",
            "  inflating: t2m/text_mot_match/eval/E020.txt  \n",
            "  inflating: t2m/text_mot_match/eval/E025.txt  \n",
            "  inflating: t2m/text_mot_match/eval/E030.txt  \n",
            "  inflating: t2m/text_mot_match/eval/E035.txt  \n",
            "  inflating: t2m/text_mot_match/eval/E040.txt  \n",
            "  inflating: t2m/text_mot_match/eval/E045.txt  \n",
            "  inflating: t2m/text_mot_match/eval/E050.txt  \n",
            "  inflating: t2m/text_mot_match/eval/E055.txt  \n",
            "  inflating: t2m/text_mot_match/eval/E060.txt  \n",
            "  inflating: t2m/text_mot_match/eval/E065.txt  \n",
            "   creating: t2m/text_mot_match/model/\n",
            "  inflating: t2m/text_mot_match/model/finest.tar  \n",
            "   creating: t2m/VQVAEV3_CB1024_CMT_H1024_NRES3/\n",
            "   creating: t2m/VQVAEV3_CB1024_CMT_H1024_NRES3/meta/\n",
            "  inflating: t2m/VQVAEV3_CB1024_CMT_H1024_NRES3/meta/mean.npy  \n",
            "  inflating: t2m/VQVAEV3_CB1024_CMT_H1024_NRES3/meta/std.npy  \n",
            "   creating: t2m/VQVAEV3_CB1024_CMT_H1024_NRES3/model/\n",
            "  inflating: t2m/VQVAEV3_CB1024_CMT_H1024_NRES3/model/finest.tar  \n",
            "Archive:  kit.zip\n",
            "   creating: kit/\n",
            "   creating: kit/text_mot_match/\n",
            "   creating: kit/text_mot_match/eval/\n",
            "  inflating: kit/text_mot_match/eval/E090.txt  \n",
            "  inflating: kit/text_mot_match/eval/E025.txt  \n",
            "  inflating: kit/text_mot_match/eval/E055.txt  \n",
            "  inflating: kit/text_mot_match/eval/E105.txt  \n",
            "  inflating: kit/text_mot_match/eval/E095.txt  \n",
            "  inflating: kit/text_mot_match/eval/E070.txt  \n",
            "  inflating: kit/text_mot_match/eval/E020.txt  \n",
            "  inflating: kit/text_mot_match/eval/E080.txt  \n",
            "  inflating: kit/text_mot_match/eval/E060.txt  \n",
            "  inflating: kit/text_mot_match/eval/E005.txt  \n",
            "  inflating: kit/text_mot_match/eval/E110.txt  \n",
            "  inflating: kit/text_mot_match/eval/E015.txt  \n",
            "  inflating: kit/text_mot_match/eval/E065.txt  \n",
            "  inflating: kit/text_mot_match/eval/E035.txt  \n",
            "  inflating: kit/text_mot_match/eval/E040.txt  \n",
            "  inflating: kit/text_mot_match/eval/E075.txt  \n",
            "  inflating: kit/text_mot_match/eval/E045.txt  \n",
            "  inflating: kit/text_mot_match/eval/E010.txt  \n",
            "  inflating: kit/text_mot_match/eval/E050.txt  \n",
            "  inflating: kit/text_mot_match/eval/E030.txt  \n",
            "  inflating: kit/text_mot_match/eval/E085.txt  \n",
            "  inflating: kit/text_mot_match/eval/E100.txt  \n",
            "  inflating: kit/text_mot_match/eval/E120.txt  \n",
            "  inflating: kit/text_mot_match/eval/E115.txt  \n",
            "   creating: kit/text_mot_match/model/\n",
            "  inflating: kit/text_mot_match/model/finest.tar  \n",
            "   creating: kit/Decomp_SP001_SM001_H512/\n",
            "   creating: kit/Decomp_SP001_SM001_H512/meta/\n",
            "  inflating: kit/Decomp_SP001_SM001_H512/meta/mean.npy  \n",
            "  inflating: kit/Decomp_SP001_SM001_H512/meta/std.npy  \n",
            "   creating: kit/Decomp_SP001_SM001_H512/model/\n",
            "  inflating: kit/Decomp_SP001_SM001_H512/model/latest.tar  \n",
            "   creating: kit/Comp_v6_KLD005/\n",
            "  inflating: kit/Comp_v6_KLD005/opt.txt  \n",
            "   creating: kit/Comp_v6_KLD005/meta/\n",
            "  inflating: kit/Comp_v6_KLD005/meta/mean.npy  \n",
            "  inflating: kit/Comp_v6_KLD005/meta/std.npy  \n",
            "   creating: kit/Comp_v6_KLD005/model/\n",
            "  inflating: kit/Comp_v6_KLD005/model/latest.tar  \n",
            "   creating: kit/length_est_bigru/\n",
            "   creating: kit/length_est_bigru/model/\n",
            "  inflating: kit/length_est_bigru/model/latest.tar  \n",
            "   creating: kit/length_est_bigru/meta/\n",
            "  inflating: kit/length_est_bigru/meta/mean.npy  \n",
            "  inflating: kit/length_est_bigru/meta/std.npy  \n",
            "   creating: kit/VQVAEV3_CB1024_CMT_H1024_NRES3/\n",
            "   creating: kit/VQVAEV3_CB1024_CMT_H1024_NRES3/meta/\n",
            "  inflating: kit/VQVAEV3_CB1024_CMT_H1024_NRES3/meta/mean.npy  \n",
            "  inflating: kit/VQVAEV3_CB1024_CMT_H1024_NRES3/meta/std.npy  \n",
            "Cleaning\n",
            "\n",
            "Downloading done!\n",
            "The smpl files will be stored in the 'body_models/smpl/' folder\n",
            "\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1INYlGA76ak_cKGzvpOV2Pe6RkYTlXTW2\n",
            "To: /content/T2M-GPT/body_models/smpl.zip\n",
            "100% 35.3M/35.3M [00:02<00:00, 15.0MB/s]\n",
            "Archive:  smpl.zip\n",
            "   creating: smpl/\n",
            "  inflating: smpl/J_regressor_extra.npy  \n",
            "  inflating: smpl/smplfaces.npy      \n",
            "  inflating: smpl/kintree_table.pkl  \n",
            "  inflating: smpl/SMPL_NEUTRAL.pkl   \n",
            "Cleaning\n",
            "\n",
            "Downloading done!\n"
          ]
        }
      ],
      "source": [
        "!bash dataset/prepare/download_model.sh\n",
        "!bash dataset/prepare/download_extractor.sh\n",
        "!bash dataset/prepare/download_smpl.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQUv_o9n3DWV",
        "outputId": "05f4c2b1-b615-4bc0-a9f2-3048268ca8d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading checkpoint from pretrained/VQVAE/net_last.pth\n",
            "loading transformer checkpoint from pretrained/VQTransformer_corruption05/net_best_fid.pth\n"
          ]
        }
      ],
      "source": [
        "print ('loading checkpoint from {}'.format(args.resume_pth))\n",
        "ckpt = torch.load(args.resume_pth, map_location='cpu')\n",
        "net.load_state_dict(ckpt['net'], strict=True)\n",
        "net.eval()\n",
        "net.cuda()\n",
        "\n",
        "print ('loading transformer checkpoint from {}'.format(args.resume_trans))\n",
        "ckpt = torch.load(args.resume_trans, map_location='cpu')\n",
        "trans_encoder.load_state_dict(ckpt['trans'], strict=True)\n",
        "trans_encoder.eval()\n",
        "trans_encoder.cuda()\n",
        "\n",
        "mean = torch.from_numpy(np.load('./checkpoints/t2m/VQVAEV3_CB1024_CMT_H1024_NRES3/meta/mean.npy')).cuda()\n",
        "std = torch.from_numpy(np.load('./checkpoints/t2m/VQVAEV3_CB1024_CMT_H1024_NRES3/meta/std.npy')).cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39nKJLUO3WED"
      },
      "source": [
        "# SMPL Class Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "f69USNqq3Z77"
      },
      "outputs": [],
      "source": [
        "class joints2smpl:\n",
        "    def __init__(self, num_frames, device_id, cuda=True):\n",
        "        self.device = torch.device(\"cuda:\" + str(device_id) if cuda else \"cpu\")\n",
        "        # self.device = torch.device(\"cpu\")\n",
        "        self.batch_size = num_frames\n",
        "        self.num_joints = 22  # for HumanML3D\n",
        "        self.joint_category = \"AMASS\"\n",
        "        self.num_smplify_iters = 150\n",
        "        self.fix_foot = False\n",
        "        print(config.SMPL_MODEL_DIR)\n",
        "        smplmodel = smplx.create(config.SMPL_MODEL_DIR,\n",
        "                                 model_type=\"smpl\", gender=\"neutral\", ext=\"pkl\",\n",
        "                                 batch_size=self.batch_size).to(self.device)\n",
        "\n",
        "        # ## --- load the mean pose as original ----\n",
        "        smpl_mean_file = config.SMPL_MEAN_FILE\n",
        "\n",
        "        file = h5py.File(smpl_mean_file, 'r')\n",
        "        self.init_mean_pose = torch.from_numpy(file['pose'][:]).unsqueeze(0).repeat(self.batch_size, 1).float().to(self.device)\n",
        "        self.init_mean_shape = torch.from_numpy(file['shape'][:]).unsqueeze(0).repeat(self.batch_size, 1).float().to(self.device)\n",
        "        self.cam_trans_zero = torch.Tensor([0.0, 0.0, 0.0]).unsqueeze(0).to(self.device)\n",
        "        #\n",
        "\n",
        "        # # #-------------initialize SMPLify\n",
        "        self.smplify = SMPLify3D(smplxmodel=smplmodel,\n",
        "                            batch_size=self.batch_size,\n",
        "                            joints_category=self.joint_category,\n",
        "                            num_iters=self.num_smplify_iters,\n",
        "                            device=self.device)\n",
        "\n",
        "\n",
        "    def npy2smpl(self, npy_path):\n",
        "        out_path = npy_path.replace('.npy', '_rot.npy')\n",
        "        motions = np.load(npy_path, allow_pickle=True)[None][0]\n",
        "        # print_batch('', motions)\n",
        "        n_samples = motions['motion'].shape[0]\n",
        "        all_thetas = []\n",
        "        for sample_i in tqdm(range(n_samples)):\n",
        "            thetas, _ = self.joint2smpl(motions['motion'][sample_i].transpose(2, 0, 1))  # [nframes, njoints, 3]\n",
        "            all_thetas.append(thetas.cpu().numpy())\n",
        "        motions['motion'] = np.concatenate(all_thetas, axis=0)\n",
        "        print('motions', motions['motion'].shape)\n",
        "\n",
        "        print(f'Saving [{out_path}]')\n",
        "        np.save(out_path, motions)\n",
        "        exit()\n",
        "\n",
        "\n",
        "\n",
        "    def joint2smpl(self, input_joints, init_params=None):\n",
        "        _smplify = self.smplify # if init_params is None else self.smplify_fast\n",
        "        pred_pose = torch.zeros(self.batch_size, 72).to(self.device)\n",
        "        pred_betas = torch.zeros(self.batch_size, 10).to(self.device)\n",
        "        pred_cam_t = torch.zeros(self.batch_size, 3).to(self.device)\n",
        "        keypoints_3d = torch.zeros(self.batch_size, self.num_joints, 3).to(self.device)\n",
        "\n",
        "        # run the whole seqs\n",
        "        num_seqs = input_joints.shape[0]\n",
        "\n",
        "\n",
        "        # joints3d = input_joints[idx]  # *1.2 #scale problem [check first]\n",
        "        keypoints_3d = torch.Tensor(input_joints).to(self.device).float()\n",
        "\n",
        "        # if idx == 0:\n",
        "        if init_params is None:\n",
        "            pred_betas = self.init_mean_shape\n",
        "            pred_pose = self.init_mean_pose\n",
        "            pred_cam_t = self.cam_trans_zero\n",
        "        else:\n",
        "            pred_betas = init_params['betas']\n",
        "            pred_pose = init_params['pose']\n",
        "            pred_cam_t = init_params['cam']\n",
        "\n",
        "        if self.joint_category == \"AMASS\":\n",
        "            confidence_input = torch.ones(self.num_joints)\n",
        "            # make sure the foot and ankle\n",
        "            if self.fix_foot == True:\n",
        "                confidence_input[7] = 1.5\n",
        "                confidence_input[8] = 1.5\n",
        "                confidence_input[10] = 1.5\n",
        "                confidence_input[11] = 1.5\n",
        "        else:\n",
        "            print(\"Such category not settle down!\")\n",
        "\n",
        "        new_opt_vertices, new_opt_joints, new_opt_pose, new_opt_betas, \\\n",
        "        new_opt_cam_t, new_opt_joint_loss = _smplify(\n",
        "            pred_pose.detach(),\n",
        "            pred_betas.detach(),\n",
        "            pred_cam_t.detach(),\n",
        "            keypoints_3d,\n",
        "            conf_3d=confidence_input.to(self.device),\n",
        "            # seq_ind=idx\n",
        "        )\n",
        "\n",
        "        thetas = new_opt_pose.reshape(self.batch_size, 24, 3)\n",
        "        thetas = geometry.matrix_to_rotation_6d(geometry.axis_angle_to_matrix(thetas))  # [bs, 24, 6]\n",
        "        root_loc = torch.tensor(keypoints_3d[:, 0])  # [bs, 3]\n",
        "        root_loc = torch.cat([root_loc, torch.zeros_like(root_loc)], dim=-1).unsqueeze(1)  # [bs, 1, 6]\n",
        "        thetas = torch.cat([thetas, root_loc], dim=1).unsqueeze(0).permute(0, 2, 3, 1)  # [1, 25, 6, 196]\n",
        "\n",
        "        return thetas.clone().detach(), {'pose': new_opt_joints[0, :24].flatten().clone().detach(), 'betas': new_opt_betas.clone().detach(), 'cam': new_opt_cam_t.clone().detach(),\n",
        "                                          'smpl_pose_72': new_opt_pose.clone().detach()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxawfbqe3lYZ"
      },
      "source": [
        "# Inference Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-vBUfr1l3pCC"
      },
      "outputs": [],
      "source": [
        "def animate_3d_motion(joints, title=None,\n",
        "                      folder_name = \"anime\", output_gif = \"output.gif\", duration=50):\n",
        "    # Check if the folder already exists or not\n",
        "    if not os.path.exists(folder_name):\n",
        "        # Create the folder\n",
        "        os.makedirs(folder_name)\n",
        "        print(f\"Folder '{folder_name}' created successfully.\")\n",
        "    else:\n",
        "        print(f\"Folder '{folder_name}' already exists.\")\n",
        "\n",
        "    # joints = xyz.detach().cpu().numpy().squeeze() # (time, num_joints, 3) No Batch\n",
        "    data = joints.copy().reshape(len(joints), -1, 3) # (time, num_joints, 3)\n",
        "    nb_joints = joints.shape[1]\n",
        "    smpl_kinetic_chain = [[0, 11, 12, 13, 14, 15], [0, 16, 17, 18, 19, 20], [0, 1, 2, 3, 4], [3, 5, 6, 7], [3, 8, 9, 10]] if nb_joints == 21 else [[0, 2, 5, 8, 11], [0, 1, 4, 7, 10], [0, 3, 6, 9, 12, 15], [9, 14, 17, 19, 21], [9, 13, 16, 18, 20]]\n",
        "\n",
        "    limits = 1000 if nb_joints == 21 else 2 # 2 or 1000\n",
        "    MINS = data.min(axis=0).min(axis=0)\n",
        "    MAXS = data.max(axis=0).max(axis=0)\n",
        "\n",
        "    colors = ['red', 'blue', 'black', 'red', 'blue',\n",
        "              'darkblue', 'darkblue', 'darkblue', 'darkblue', 'darkblue',\n",
        "              'darkred', 'darkred', 'darkred', 'darkred', 'darkred']\n",
        "    frame_number = data.shape[0]\n",
        "\n",
        "    height_offset = MINS[1]\n",
        "    data[:, :, 1] -= height_offset\n",
        "    trajec = data[:, 0, [0, 2]] # [[0. 0.]] or tensor with shape (100, 2)\n",
        "\n",
        "    data[..., 0] -= data[:, 0:1, 0]\n",
        "    data[..., 2] -= data[:, 0:1, 2]\n",
        "\n",
        "    def update(index):\n",
        "        def init():\n",
        "            ax.set_xlim(-limits, limits)\n",
        "            ax.set_ylim(-limits, limits)\n",
        "            ax.set_zlim(0, limits)\n",
        "            ax.grid(b=False)\n",
        "\n",
        "        def plot_xzPlane(minx, maxx, miny, minz, maxz):\n",
        "            ## Plot a plane XZ\n",
        "            verts = [\n",
        "                [minx, miny, minz],\n",
        "                [minx, miny, maxz],\n",
        "                [maxx, miny, maxz],\n",
        "                [maxx, miny, minz]\n",
        "            ]\n",
        "            xz_plane = Poly3DCollection([verts])\n",
        "            xz_plane.set_facecolor((0.5, 0.5, 0.5, 0.5))\n",
        "            ax.add_collection3d(xz_plane)\n",
        "\n",
        "        fig = plt.figure(figsize=(480/96., 320/96.), dpi=96) if nb_joints == 21 else plt.figure(figsize=(10, 10), dpi=96) # Figure (960, 960)\n",
        "\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        init()\n",
        "\n",
        "        ax.view_init(elev=110, azim=-90) # Set the elevation and azimuth of the axes in degrees (not radians).\n",
        "        ax.dist = 7.5\n",
        "\n",
        "        plot_xzPlane(MINS[0] - trajec[index, 0], MAXS[0] - trajec[index, 0], 0, MINS[2] - trajec[index, 1],\n",
        "                      MAXS[2] - trajec[index, 1])\n",
        "\n",
        "        if index > 1:\n",
        "            ax.plot3D(trajec[:index, 0] - trajec[index, 0], np.zeros_like(trajec[:index, 0]),\n",
        "                      trajec[:index, 1] - trajec[index, 1], linewidth=1.0,\n",
        "                      color='blue')\n",
        "\n",
        "        for i, (chain, color) in enumerate(zip(smpl_kinetic_chain, colors)):\n",
        "            if i < 5:\n",
        "                linewidth = 4.0\n",
        "            else:\n",
        "                linewidth = 2.0\n",
        "            ax.plot3D(data[index, chain, 0], data[index, chain, 1], data[index, chain, 2], linewidth=linewidth,\n",
        "                      color=color)\n",
        "\n",
        "        plt.axis('off')\n",
        "        ax.set_xticklabels([])\n",
        "        ax.set_yticklabels([])\n",
        "        ax.set_zticklabels([])\n",
        "        frame_name = str(index)+\".png\"\n",
        "        plt.savefig(os.path.join(folder_name, frame_name))\n",
        "        plt.close()\n",
        "        return\n",
        "\n",
        "    for i in range(frame_number):\n",
        "        update(i)\n",
        "\n",
        "    ## All frames are generated.\n",
        "\n",
        "    # List all PNG files in the folder\n",
        "    png_files = [f for f in os.listdir(folder_name) if f.endswith(\".png\")]\n",
        "    # Sort the files to ensure they are in the desired order\n",
        "    png_files = sorted(png_files, key=lambda x: int(x.split(\".\")[0]))\n",
        "\n",
        "    # Create a list to store the images\n",
        "    images = []\n",
        "    # Load each PNG file and append it to the images list\n",
        "    for png_file in png_files:\n",
        "        image_path = os.path.join(folder_name, png_file)\n",
        "        img = Image.open(image_path)\n",
        "        images.append(img)\n",
        "\n",
        "    # Save the images as a GIF\n",
        "    images[0].save(output_gif, save_all=True, append_images=images[1:], duration=duration, loop=0)\n",
        "\n",
        "    try:\n",
        "        # Use shutil.rmtree to delete the folder and its contents\n",
        "        shutil.rmtree(folder_name)\n",
        "        print(f\"Folder '{folder_name}' and its contents have been deleted successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error deleting folder: {e}\")\n",
        "\n",
        "# animate_3d_motion(joints = xyz.detach().cpu().numpy().squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvdG73tc39sr"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r530R-b94NLL"
      },
      "source": [
        "Expected output files for Patrick:\n",
        "```\n",
        "motion_{i}.npy\n",
        "output_{i}.gif\n",
        "motion_smpl_72_{i}.npy\n",
        "```  \n",
        "Patick's Note: __How to obtain the mean and standard deviation data (npy format)__  \n",
        "These authors directly used the files extracted by T2M, because they are the mean and std for the pre-trained motion extractor. According to their code, these files are generated from the HumanML3D training dataset but with some rules, we can find their code below.  \n",
        "https://github.com/EricGuo5513/text-to-motion/blob/main/data/dataset.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ghMBUGiw3z57"
      },
      "outputs": [],
      "source": [
        "#@title Prompts for XXXX\n",
        "prompt_list = [\n",
        "    \"A person half kneel with one leg to work near the floor\",\n",
        "    \"A person half squat to work near the floor\",\n",
        "    \"A person raise both hands above his head and keep them there\",\n",
        "    \"A person squat to carry up something\",\n",
        "    \"A person move a box from left to right\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbODq8w351un",
        "outputId": "6fa82555-8b81-4977-9f89-b3ee0ae97093"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rprocessing...:   0%|          | 0/50 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./body_models/\n",
            "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
            "Folder 'anime' created successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rprocessing...:   2%|▏         | 1/50 [01:59<1:37:52, 119.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Folder 'anime' and its contents have been deleted successfully.\n",
            "./body_models/\n",
            "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
            "Folder 'anime' created successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rprocessing...:   4%|▍         | 2/50 [03:41<1:27:23, 109.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Folder 'anime' and its contents have been deleted successfully.\n",
            "./body_models/\n",
            "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
            "Folder 'anime' created successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rprocessing...:   6%|▌         | 3/50 [05:30<1:25:20, 108.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Folder 'anime' and its contents have been deleted successfully.\n",
            "./body_models/\n",
            "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
            "Folder 'anime' created successfully.\n"
          ]
        }
      ],
      "source": [
        "for i, prompt in enumerate(prompt_list):\n",
        "    n_output = 50 # number of outputs\n",
        "    bookmark = 0 # bookmark to continue colab work\n",
        "\n",
        "    text = clip.tokenize(prompt, truncate=True).cuda()\n",
        "    feat_clip_text = clip_model.encode_text(text).float()\n",
        "\n",
        "    save_folder_dir = \"/content/drive/MyDrive/text2pose\"\n",
        "    folder_name = prompt.replace(\" \", \"_\")\n",
        "    folder_path = os.path.join(save_folder_dir, folder_name)\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "    for j in tqdm(range(0, n_output), desc=\"processing...\"):\n",
        "        index_motion = trans_encoder.sample(feat_clip_text[0:1], True)\n",
        "        pred_pose = net.forward_decoder(index_motion)\n",
        "\n",
        "        pred_xyz = recover_from_ric((pred_pose*std+mean).float(), 22)\n",
        "        xyz = pred_xyz.reshape(1, -1, 22, 3)\n",
        "        motion = xyz.detach().cpu().numpy()\n",
        "\n",
        "        ## motion xyz save\n",
        "        motion_filename = 'motion_'+str(j)+'.npy'\n",
        "        save_motion_dir = os.path.join(folder_path, motion_filename)\n",
        "        np.save(save_motion_dir, motion)\n",
        "\n",
        "        # frames, njoints, nfeats = xyz.detach().cpu().numpy().shape\n",
        "        j2s = joints2smpl(num_frames=motion.shape[1], device_id=0, cuda=True)\n",
        "        rot2xyz = Rotation2xyz(device=torch.device(\"cuda:0\"))\n",
        "        motion_tensor, opt_dict = j2s.joint2smpl(motion[0])\n",
        "\n",
        "        ## smpl save\n",
        "        smpl_filename = f'smpl_pose_72_{str(j)}.npy'\n",
        "        save_smpl_dir = os.path.join(folder_path, smpl_filename)\n",
        "\n",
        "        smpl_array = opt_dict['smpl_pose_72'].cpu().numpy()\n",
        "        np.save(save_smpl_dir, smpl_array)\n",
        "\n",
        "        ## gif save\n",
        "        gif_filename = 'output_'+str(j)+'.gif'\n",
        "        save_gif_dir = os.path.join(folder_path, gif_filename)\n",
        "        animate_3d_motion(joints = motion.squeeze(), output_gif=save_gif_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Mass Visualization\n",
        "prompt = prompt_list[3]\n",
        "\n",
        "folder_name = prompt.replace(\" \", \"_\")\n",
        "folder_dir = \"/content/drive/MyDrive/text2pose\"\n",
        "folder_path = os.path.join(folder_dir, folder_name)\n",
        "\n",
        "motion_files = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if filename.startswith(\"motion_\") and filename.endswith(\".npy\")]\n",
        "motion_files.sort(key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
        "motion_files = [np.load(motion) for motion in motion_files]\n",
        "# motion file shape [1, # of frames, # of body joints (22), 3]\n",
        "\n",
        "def animate_3d_motions(motions, grid_size=(3, 3), dist=3.0, ax_dist=7, title=None, duration=50, prompt=prompt):\n",
        "    if not os.path.exists(\"temp_frames\"):\n",
        "        os.makedirs(\"temp_frames\")\n",
        "        print(\"Folder 'temp_frames' created successfully.\")\n",
        "    else:\n",
        "        print(\"Folder 'temp_frames' already exists.\")\n",
        "    temp_folder_name = \"temp_frames\"\n",
        "\n",
        "    motions = [motion.squeeze() for motion in motions]  # List of motions (numpy arrays)\n",
        "\n",
        "    nb_frames = max(len(motion) for motion in motions)\n",
        "    nb_motions = len(motions)\n",
        "    nb_rows, nb_cols = grid_size\n",
        "\n",
        "    # Pad shorter motions with the last frame to match the length of the longest motion\n",
        "    for i in range(nb_motions):\n",
        "        if len(motions[i]) < nb_frames:\n",
        "            pad_frames = np.tile(motions[i][-1], (nb_frames - len(motions[i]), 1, 1))\n",
        "            motions[i] = np.vstack((motions[i], pad_frames))\n",
        "\n",
        "    height_axis = 2\n",
        "    height_offset = min(motion[0, :, height_axis].min() for motion in motions)\n",
        "    height_rescaled_motion = motions.copy()\n",
        "    for i, motion in enumerate(motions):\n",
        "        height_rescaled_motion[i][:,:,height_axis] -= height_offset\n",
        "\n",
        "    grid_rows, grid_cols = grid_size\n",
        "    distance_x, distance_y, distance_z = dist, dist, dist\n",
        "    x_spare = dist/grid_cols\n",
        "\n",
        "    data = np.zeros((height_rescaled_motion[0].shape[0], 22 * grid_rows * grid_cols, 3))\n",
        "\n",
        "    # Iterate through the motions and place them in the grid\n",
        "    for i, motion in enumerate(height_rescaled_motion):\n",
        "        row_idx = i // grid_rows\n",
        "        col_idx = i % grid_cols\n",
        "\n",
        "        # Calculate the offset for each motion\n",
        "        offset_x = col_idx * distance_x + row_idx * x_spare\n",
        "        offset_y = 0 # row_idx * distance_z\n",
        "        offset_z = row_idx * distance_z  # Assuming all motions are on the same z-plane\n",
        "        # Apply the offset to the motion\n",
        "        motion_with_offset = motion + np.array([offset_x, offset_y, offset_z])\n",
        "        # Insert the motion into the data array\n",
        "        data[:, i * 22:(i + 1) * 22, :] = motion_with_offset\n",
        "\n",
        "    frame_number = data.shape[0]\n",
        "    nb_joints = data.shape[1]\n",
        "    smpl_kinetic_chain = [[0, 11, 12, 13, 14, 15], [0, 16, 17, 18, 19, 20], [0, 1, 2, 3, 4], [3, 5, 6, 7], [3, 8, 9, 10]] if nb_joints == 21 \\\n",
        "                          else [[0, 2, 5, 8, 11], [0, 1, 4, 7, 10], [0, 3, 6, 9, 12, 15], [9, 14, 17, 19, 21], [9, 13, 16, 18, 20]]\n",
        "    num_persons = len(motions)\n",
        "    smpl_kinetic_chain_list = []\n",
        "\n",
        "    for p in range(num_persons):\n",
        "        for group in smpl_kinetic_chain:\n",
        "            temp = []\n",
        "            for joint_idx in group:\n",
        "                temp.append(joint_idx+22*p)\n",
        "            smpl_kinetic_chain_list.append(temp)\n",
        "\n",
        "    colors = [\"red\", \"blue\", \"black\", \"red\", \"blue\"]*num_persons # color order: [left_leg, right_leg, body, left_arm, right_arm]\n",
        "    verts = [[-1, 0, -1],\n",
        "             [-1, 0, dist*grid_rows],\n",
        "             [dist*grid_rows, 0, dist*grid_rows+1],\n",
        "             [dist*grid_rows, 0, -1]]\n",
        "\n",
        "    for frame in range(frame_number):\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        ax.view_init(elev=95, azim=-90)\n",
        "        ax.dist = ax_dist\n",
        "        ax.set_xlim(-1, dist*grid_rows)\n",
        "        ax.set_ylim(0, 2)\n",
        "        ax.set_zlim(-1, dist*grid_rows)\n",
        "        ax.set_box_aspect([dist*grid_rows+1, 2, dist*grid_rows+1])\n",
        "        ax.grid(b=False)\n",
        "        # ax.set_xlabel('X Label')\n",
        "        # ax.set_ylabel('Y Label')\n",
        "        # ax.set_zlabel('Z Label')\n",
        "        plt.axis('off')\n",
        "        ax.set_xticklabels([])\n",
        "        ax.set_yticklabels([])\n",
        "        ax.set_zticklabels([])\n",
        "\n",
        "        xz_plane = Poly3DCollection([verts])\n",
        "        xz_plane.set_facecolor((0.5, 0.5, 0.5, 0.2))\n",
        "        ax.add_collection3d(xz_plane)\n",
        "        for chain, color in zip(smpl_kinetic_chain_list, colors):\n",
        "            ax.plot3D(data[frame, chain, 0], data[frame, chain, 1], data[frame, chain, 2], linewidth=2, color=color)\n",
        "        \n",
        "        frame_name = str(frame)+\".png\"\n",
        "        plt.savefig(os.path.join(temp_folder_name, frame_name))\n",
        "        plt.close()\n",
        "\n",
        "    png_files = [f for f in os.listdir(temp_folder_name) if f.endswith(\".png\")]\n",
        "    # Sort the files to ensure they are in the desired order\n",
        "    png_files = sorted(png_files, key=lambda x: int(x.split(\".\")[0]))\n",
        "\n",
        "    # Create a list to store the images\n",
        "    images = []\n",
        "    # Load each PNG file and append it to the images list\n",
        "    for png_file in png_files:\n",
        "        image_path = os.path.join(temp_folder_name, png_file)\n",
        "        img = Image.open(image_path)\n",
        "        images.append(img)\n",
        "\n",
        "    file_name = prompt.replace(\" \", \"_\")+\"_mass.gif\"\n",
        "    file_path = os.path.join(folder_dir, file_name)\n",
        "    # Save the images as a GIF\n",
        "    images[0].save(file_path, save_all=True, append_images=images[1:], duration=duration, loop=0)\n",
        "\n",
        "    try:\n",
        "        # Use shutil.rmtree to delete the folder and its contents\n",
        "        shutil.rmtree(temp_folder_name)\n",
        "        print(f\"Folder '{temp_folder_name}' and its contents have been deleted successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error deleting folder: {e}\")\n",
        "\n",
        "motion_files = motion_files[:9]\n",
        "animate_3d_motions(motion_files, grid_size=(3, 3), dist=3, ax_dist=7, prompt=prompt)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "jrlqSWBT2A-n",
        "39nKJLUO3WED",
        "oxawfbqe3lYZ"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
